import requests
import pandas as pd
import os
import sqlite3
import logging
import time
from dotenv import load_dotenv

# 1. í™˜ê²½ë³€ìˆ˜ ë° ë¡œê¹… ì„¤ì •
load_dotenv() # .env íŒŒì¼ ë¡œë“œ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

class KakaoLandmarkMapper:
    def __init__(self):
        # .envì—ì„œ MAP_API_KEY ê°€ì ¸ì˜¤ê¸°
        self.api_key = os.getenv("MAP_API_KEY")
        
        if not self.api_key:
            raise ValueError("âŒ .env íŒŒì¼ì— 'MAP_API_KEY'ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        self.headers = {"Authorization": f"KakaoAK {self.api_key}"}
        self.base_url = "https://dapi.kakao.com/v2/local/search/keyword.json"
        self.db_path = "database/subway.db"
        
        # ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ ë° í‚¤ì›Œë“œ ì •ì˜ (ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì ìš©)
        self.search_priorities = [
            {'type': 'UnivStudent', 'keyword': 'ëŒ€í•™êµ', 'code': 'SC4'}, # í•™êµ
            {'type': 'Transfer', 'keyword': 'í™˜ìŠ¹ì„¼í„°', 'code': ''},     # êµí†µ
            {'type': 'Senior', 'keyword': 'ì¢…í•©ë³‘ì›', 'code': 'HP8'},    # ë³‘ì›
            {'type': 'Senior', 'keyword': 'ì „í†µì‹œì¥', 'code': ''},       # ì‹œì¥
            {'type': 'Office', 'keyword': 'êµ¬ì²­', 'code': 'PO3'},        # ê´€ê³µì„œ
            {'type': 'Office', 'keyword': 'ì‚°ì—…ë‹¨ì§€', 'code': ''},       # ì˜¤í”¼ìŠ¤
            {'type': 'Hotplace', 'keyword': 'ë¨¹ìê³¨ëª©', 'code': 'FD6'}   # ìŒì‹ì  ë°€ì§‘
        ]

    def get_stations_from_db(self):
        """
        [í•µì‹¬] ì‹¤ì œ DBì— ì ì¬ëœ ì—­ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¡°íšŒ (ê°€ì • X)
        """
        if not os.path.exists(self.db_path):
            logging.error(f"âŒ DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
            return []

        conn = sqlite3.connect(self.db_path)
        try:
            # raw_station_history í…Œì´ë¸”ì—ì„œ ì¤‘ë³µ ì—†ì´ ì—­ ì´ë¦„ ì¶”ì¶œ
            # (ë§Œì•½ raw í…Œì´ë¸”ì´ ë¹„ì–´ìˆìœ¼ë©´ loader.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•¨)
            query = "SELECT DISTINCT station_name FROM raw_station_history"
            df = pd.read_sql(query, conn)
            
            stations = df['station_name'].tolist()
            logging.info(f"ğŸ“‚ DBì—ì„œ ì´ {len(stations)}ê°œì˜ ì—­ ì •ë³´ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return stations
            
        except Exception as e:
            logging.error(f"DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
        finally:
            conn.close()

    def search_station_landmark(self, station_name):
        """
        íŠ¹ì • ì—­ ì£¼ë³€ 500m ë°˜ê²½ì˜ ì‹œì„¤ë¬¼ ê²€ìƒ‰
        """
        best_target = 'General'
        found_feats = []

        # ì—­ ì´ë¦„ ë³´ì • (ex: 'ì„œìš¸' -> 'ì„œìš¸ì—­', 'ê°•ë‚¨' -> 'ê°•ë‚¨ì—­')
        search_query_base = station_name if station_name.endswith('ì—­') else f"{station_name}ì—­"

        for item in self.search_priorities:
            # ê²€ìƒ‰ì–´: "ê°•ë‚¨ì—­ ëŒ€í•™êµ", "ì¢…ë¡œ3ê°€ì—­ ì¢…í•©ë³‘ì›" ë“±
            query = f"{search_query_base} {item['keyword']}"
            params = {
                'query': query, 
                'category_group_code': item['code'], 
                'radius': 500, # ë°˜ê²½ 500m ë‚´
                'size': 1      # 1ê°œë§Œ ìˆì–´ë„ í•´ë‹¹ íŠ¹ì§• ë³´ìœ ë¡œ ì¸ì •
            }
            
            try:
                response = requests.get(self.base_url, headers=self.headers, params=params)
                
                if response.status_code == 200:
                    documents = response.json().get('documents')
                    if documents:
                        # íŠ¹ì§• ë°œê²¬
                        feat_name = item['keyword']
                        found_feats.append(feat_name)
                        
                        # ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ íƒ€ê²Ÿ ì„¤ì • (ì•„ì§ Generalì¼ ê²½ìš°ì—ë§Œ)
                        if best_target == 'General':
                            best_target = item['type']
                            
            except Exception as e:
                logging.error(f"API ìš”ì²­ ì¤‘ ì—ëŸ¬: {e}")
            
            time.sleep(0.05) # API ë¶€í•˜ ë°©ì§€ (ì´ˆë‹¹ 20íšŒ ì œí•œ ê³ ë ¤)

        # ì•„ë¬´ íŠ¹ì§•ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        return best_target, "|".join(found_feats)

    def run(self):
        # 1. DBì—ì„œ ì—­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        stations = self.get_stations_from_db()
        
        if not stations:
            logging.warning("ë¶„ì„í•  ì—­ì´ ì—†ìŠµë‹ˆë‹¤. etl/loader.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        results = []
        total = len(stations)
        
        logging.info(f"ğŸš€ ì¹´ì¹´ì˜¤ API ê¸°ë°˜ ëœë“œë§ˆí¬ ë¶„ì„ ì‹œì‘ (ì´ {total}ê°œ ì—­)")
        
        for idx, station in enumerate(stations):
            target, features = self.search_station_landmark(station)
            
            results.append({
                'station_name': station,
                'main_target': target,
                'feature_list_kakao': features
            })
            
            # ì§„í–‰ìƒí™© ë¡œê¹… (10ê°œ ë‹¨ìœ„)
            if (idx + 1) % 10 == 0:
                print(f"[{idx + 1}/{total}] ì²˜ë¦¬ ì¤‘... ({station}: {target})")

        # ê²°ê³¼ DataFrame ìƒì„±
        df_result = pd.DataFrame(results)
        
        # 2. ê²°ê³¼ ì €ì¥ (CSV) -> ì¶”í›„ DB ì ì¬ìš©
        os.makedirs("data", exist_ok=True)
        save_path = "data/station_kakao_feature.csv"
        df_result.to_csv(save_path, index=False, encoding="utf-8-sig")
        
        logging.info(f"âœ… ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì €ì¥ë¨: {save_path}")
        print(df_result.head())

if __name__ == "__main__":
    try:
        mapper = KakaoLandmarkMapper()
        mapper.run()
    except Exception as e:
        logging.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")